{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Statistical hypothesis tests in Python.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOFmA38g+BMJqSJKO/2T/gM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorviro/Machine-Learning-Python/blob/master/Statistical_hypothesis_tests_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIFxMrMLUdM5"
      },
      "source": [
        "# Table of contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JABI_qFSUgf4"
      },
      "source": [
        "\n",
        "1. [Introduction](#1)\n",
        "    1. [Hypothesis testing](#1.1)\n",
        "        1. [Example: Independent two-sample t-test](#1.1.1)\n",
        "        2. [Statistical test interpretation](#1.1.2)\n",
        "            1. [Critical values](#1.1.2.1)\n",
        "            1. [P-values](#1.1.2.2)\n",
        "    2. [Confidence intervals](#1.2)\n",
        "2. [Summary statistical tests](#2)\n",
        "    1. [Inferences about the means](#2.1)\n",
        "        1. [Analysis of variance](#2.1.1)\n",
        "            1. [Post-hoc analysis](#2.1.1.1)\n",
        "    2. [Inferences about the variances](#2.2)\n",
        "        1. [Inferences about the variances of normal distributions ](#2.2.1)\n",
        "        2. [Statistical tests for equality of variance](#2.2.2)\n",
        "    3. [Normality tests](#2.3)\n",
        "    4. [Nonparametric tests](#2.4)\n",
        "    5. [Correlation tests](#2.5)\n",
        "3. [References](#3)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acxsR3X52Kpz"
      },
      "source": [
        "# 1 Introduction <a name=\"1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2TO8rF12MZ1"
      },
      "source": [
        "In statistics, when we wish to start asking **questions about the data** and interpret the results, we use statistical methods that provide a confidence or likelihood about the answers. In general, this class of methods is called [**statistical hypothesis testing**](https://en.wikipedia.org/wiki/Statistical_hypothesis_testing), or significance tests.\n",
        "\n",
        "In this notebook, we explain the key concepts of statistical hypothesis testing. Then, we summary the most used statistical tests in machine learning. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txK7UTcw41Q3"
      },
      "source": [
        "## 1.1 Hypothesis testing <a name=\"1.1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHoWey14ys0Q"
      },
      "source": [
        "A statistical hypothesis is a **statement** or assumption either about the parameters of a probability distribution or the parameters of a model. The hypothesis reflects some conjecture about the problem situation. The result of the test allows us to **interpret whether the assumption holds or whether the assumption has been violated**.\n",
        "\n",
        "Some concrete examples might be:\n",
        "\n",
        "- A test that assumes that data follows a normal distribution.\n",
        "- A test that assumes that 2 independent samples have identical average.\n",
        "\n",
        "\n",
        "Let's think about the last example. This may be stated formally as \n",
        "\n",
        "$$\n",
        "H_0: \\mu_1=\\mu_2\\\\\n",
        "H_1: \\mu_1\\neq\\mu_2\n",
        "$$\n",
        "\n",
        "where $\\mu_1$ is the mean of the first sample and $\\mu_2$ is the mean of the second sample. The statement $H_0: \\mu_1=\\mu_2$ is called the **null hypothesis** and $H_1: \\mu_1\\neq\\mu_2$ is called the **alternative hypothesis**. The alternative hypothesis specified here is called a two-sided alternative hypothesis because it would be true if $\\mu_1<\\mu_2$ or if $\\mu_1>\\mu_2$.\n",
        "\n",
        "Two kinds of errors may be committed when testing hypotheses. If the null hypothesis is rejected when it is true, a type I error has occurred. If the null hypothesis is not rejected when it is false, a type II error has been made. The probabilities of these two errors are given\n",
        "special symbols\n",
        "\n",
        "$$\n",
        "\\alpha=P(type I error)\\\\\n",
        "\\beta=P(type II error)\n",
        "$$\n",
        "\n",
        "The general procedure in hypothesis testing is to specify a value of the probability of type I error $\\alpha$, often called the **significance level** of the test, and then design the test procedure so that the probability of type II error $\\beta$ has a suitably small value.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvhJaw_a1Lz8"
      },
      "source": [
        "### 1.1.1 Example: Independent two-Sample t-Test <a name=\"1.1.1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqSDSYOQ1L5V"
      },
      "source": [
        "Suppose that we could assume that the variances were identical for both samples. Then the appropriate test statistic to use for comparing two sample means is\n",
        "\n",
        "$t_o=\\frac{\\overline{y_1}-\\overline{y_2}}{S_p\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}}\\sim t_{n_1+n_2-2}$\n",
        "\n",
        "where $\\overline{y_1}$ and $\\overline{y_2}$ are the sample means, $n_1$ and $n_2$ are the sample sizes, $S^2_p$ is an estimate of the common variance $\\sigma^2_1=\\sigma^2_2=\\sigma^2$, \n",
        "and $t_{n_1+n_2-2}$ is the $t$ distribution with $n_1+n_2-2$ degrees of freedom.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gibdYxzuZdhb"
      },
      "source": [
        "### 1.1.2 Statistical Test Interpretation <a name=\"1.1.2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh4Sb1aqZetp"
      },
      "source": [
        "There are two common forms to interpret a statistical hypothesis test, and they must be interpreted in different ways. They are the critical values and p-value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toCQExZNnUGu"
      },
      "source": [
        "#### 1.1.2.1 Critical values <a name=\"1.1.2.1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_JOd_VbnWSa"
      },
      "source": [
        "A statistic calculated by a statistical hypothesis test can be interpreted using critical values from the distribution of the test statistic.\n",
        "\n",
        "A [**critical value**](https://en.wikipedia.org/wiki/Critical_value) is defined in the context of the population distribution and a probability.\n",
        "\n",
        "$$P(X\\leq \\text{critical value})=p$$\n",
        "\n",
        "Where $X$ are observations from the population, $\\text{critical value}$ is the calculated critical value, and $p$ is the chosen probability.\n",
        "\n",
        "Critical values are calculated using a mathematical function where the probability is provided as an argument. For most common distributions, the value cannot be calculated analytically; instead, it must be estimated using numerical methods. Historically it is common for tables of pre-calculated critical values to be provided in the appendices of statistics textbooks for reference purposes.\n",
        "\n",
        "When critical values are used in statistical significance testing, the probability is often expressed as a significance, denoted as $\\alpha$, which is the inverted probability.\n",
        "\n",
        "$$p=1-\\alpha$$\n",
        "\n",
        "Calculated critical values are used as a threshold for interpreting the result of a statistical test.\n",
        "\n",
        "The observation values in the population beyond the critical value are often called the **critical region** or the *“region of rejection\"*.\n",
        "\n",
        "**One-Tailed Test**\n",
        "\n",
        "A one-tailed test has a single critical value, such as on the left or the right of the distribution. Often, a one-tailed test has a critical value on the right of the distribution for non-symmetrical distributions (such as the Chi-Squared distribution).\n",
        "\n",
        "The statistic is compared to the calculated critical value. If the statistic is less than or equal to the critical value, we fail to reject the null hypothesis. Otherwise, it is rejected.\n",
        "\n",
        "We can summarize this interpretation as follows:\n",
        "\n",
        "- $\\text{Test Statistic}\\leq \\text{Critical Value}$: Fail to reject the null hypothesis of the statistical test.\n",
        "- $\\text{Test Statistic}> \\text{Critical Value}$: Reject the null hypothesis of the statistical test.\n",
        "\n",
        "**Two-Tailed Test**\n",
        "\n",
        "A two-tailed test has two critical values, one on each side of the distribution, which is often assumed to be symmetrical (e.g. Gaussian and Student-t distributions).\n",
        "\n",
        "When using a two-tailed test, a significance level (or alpha) used in the calculation of the critical values must be divided by 2. The critical value will then use a portion of this alpha on each side of the distribution.\n",
        "To make this concrete, consider an alpha of 5%. This would be split to give two alpha values of 2.5% on either side of the distribution with an acceptance area in the middle of the distribution of 95%.\n",
        "\n",
        "We can refer to each critical value as the lower and upper critical values for the left and right of the distribution respectively. Test statistic values more than or equal to the lower critical value and less than or equal to the upper critical value indicate the failure to reject the null hypothesis. Whereas test statistic values less than the lower critical value and more than the upper critical value indicates rejection of the null hypothesis for the test.\n",
        "\n",
        "We can summarize this interpretation as follows:\n",
        "\n",
        "- $\\text{Lower CR} \\leq \\text{Test Statistic} \\leq \\text{Upper CR}$: Failure to reject the null hypothesis of the statistical test.\n",
        "\n",
        "- $\\text{Test Statistic} < \\text{Lower CR} \\text{ OR } \\text{Test Statistic} > \\text{Upper CR}$: Reject the null hypothesis of the statistical test.\n",
        "\n",
        "If the distribution of the test statistic is symmetric around a mean of zero, then we can shortcut the check by comparing the absolute value of the test statistic to the upper critical value.\n",
        "\n",
        "- $|\\text{Test Statistic}| \\leq \\text{Upper Critical Value}$: Failure to reject the null hypothesis of the statistical test.\n",
        "\n",
        "Where $|\\text{Test Statistic}|$ is the absolute value of the calculated test statistic.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5v4fGr-sgWX"
      },
      "source": [
        "In our two sample $t$-test example, to determine whether to reject $H_0$, we would compare the value of the test statistic, $t_0$, to the upper critical value. If $|t_0|> t_{\\frac{\\alpha}{2},n_1+n_2-2}$, where $t_{\\frac{\\alpha}{2},n_1+n_2-2}$ is the upper $\\frac{\\alpha}{2}$ percentage point of the $t$ distribution with $n_1+n_2-2$ degrees of freedom, we would reject $H_0$ and conclude that the mean of the two samples differ. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmcA214k1GH6"
      },
      "source": [
        "#### 1.1.2.2 P-values <a name=\"1.1.2.2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U0iueUP1JOC"
      },
      "source": [
        "The [**P-value**](https://en.wikipedia.org/wiki/P-value) is the **probability that the test statistic will take on a value that is possible** (or that is at least as extreme as the observed value of the statistic) **when the null hypothesis $H_0$ is true**. In simple terms, this value helps us to differentiate results caused by the randomness of statistically significant results. More formally, we define the P-value as the smallest level of significance that would lead to rejection of the null hypothesis $H_0$. It is not always easy to compute the exact P-value for a test. However, most modern computer programs for statistical analysis report P-values.\n",
        "\n",
        "If $\\text{p-value}<\\alpha$ then the null hypothesis $H_0$ would be rejected at level of significance $\\alpha$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ama00f7W0l1w"
      },
      "source": [
        "## 1.2 Confidence Intervals <a name=\"1.2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpVAQtse005p"
      },
      "source": [
        "Although hypothesis testing is a useful procedure, it sometimes does not tell the entire story. It is often preferable to provide an interval within which the value of the parameter or parameters in question would be expected to lie. These interval statements are called **confidence intervals**. \n",
        "\n",
        "To define a confidence interval, suppose that $\\theta$ is an unknown parameter. To obtain an interval estimate of $\\theta$, we need to find two statistics $L$ and $U$ such that the probability statement \n",
        "\n",
        "$$P(L\\leq \\theta\\leq U)=1-\\alpha$$\n",
        "\n",
        "is true. The interval \n",
        "\n",
        "$$L\\leq \\theta\\leq U$$\n",
        " \n",
        "is called a $100(1-\\alpha$) **percent confidence interval** for the parameter $\\theta$. The interpretation of this interval is that if, in repeated random samplings, a large number of such intervals are constructed, $100(1-\\alpha)$ percent of them will contain the true value of $\\theta$ . The statistics $L$ and $U$ are called the lower and upper confidence limits, respectively, and $1-\\alpha$ is called the confidence coefficient. \n",
        "\n",
        "Suppose that we wish to find a $100(1-\\alpha$) percent confidence interval on the true difference in means $\\mu_1-\\mu_2$ for the previous example. The interval can be derived in the following way. The test statistic is\n",
        "\n",
        "$$\\frac{\\overline{y_1}-\\overline{y_2}-(\\mu_1-\\mu_2)}{S_p\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}}\\sim t_{n_1+n_2-2}$$\n",
        "\n",
        "Then we could prove that the $100(1-\\alpha$) percent confidence interval for $\\mu_1-\\mu_2$ is\n",
        "\n",
        "$$\\overline{y_1}-\\overline{y_2}-t_{n_1+n_2-2}S_p\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}\\leq \\mu_1-\\mu_2 \\leq \\overline{y_1}-\\overline{y_2}+t_{n_1+n_2-2}S_p\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}$$\n",
        "\n",
        "The actual 95 percent confidence interval estimate for the difference in mean for the two samples of the previous example is found by substituting in the equation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl48P0jSUQE8"
      },
      "source": [
        "# 2 Summary statistical tests <a name=\"2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ5AWPuedu7F"
      },
      "source": [
        "Although there are hundreds of statistical hypothesis tests that we could use, there is only a small subset that we may need to use in machine learning. \n",
        "\n",
        "- Some tests can be used in the exploratory data analysis phase to better understand the relationship between variables.\n",
        "\n",
        "- Once a machine learning model is deployed in production, a practical approach to monitor its performance degradation is *drift detection*. It suggests that if the data distribution diverges between the training and testing phases on one side and the development phase on the other, it is a strong signal that the model’s performance won’t be the same. It's common the usage of statistical tests to check if the distribution of the features does not vary between these two phases.\n",
        "\n",
        "In this section, we will discover the most popular statistical hypothesis tests used in machine learning with examples using Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjnNmDxIgYFH"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOoq0mW941U7"
      },
      "source": [
        "## 2.1 Inferences about the means <a name=\"2.1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeSEZTbugKXq"
      },
      "source": [
        "Before we look at specific significance tests, let’s first define a test dataset that we can use to demonstrate each test.\n",
        "\n",
        "We will generate two samples drawn from different distributions. Each sample will be drawn from a Gaussian distribution. We will use the `randn()` NumPy function to generate a sample of 100 Gaussian random numbers in each sample with a mean of 0 and a standard deviation of 1. Observations in the first sample are scaled to have a mean of 50 and a standard deviation of 5. Observations in the second sample are scaled to have a mean of 51 and a standard deviation of 5.\n",
        "\n",
        "We expect the statistical tests to discover that the mean of two samples are not equal, although the small sample size of 100 observations per sample will add some noise to this decision.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kftx2XbXgVdt",
        "outputId": "b1ce690c-3b78-4a0b-898b-b15120bd33a2"
      },
      "source": [
        "# seed the random number generator\n",
        "np.random.seed(1)\n",
        "# generate two sets of univariate observations\n",
        "data1 = 5 * np.random.randn(100) + 50\n",
        "data2 = 5 * np.random.randn(100) + 51\n",
        "\n",
        "# summarize\n",
        "print('data1: mean=%.3f stdv=%.3f' % (np.mean(data1), np.std(data1)))\n",
        "print('data2: mean=%.3f stdv=%.3f' % (np.mean(data2), np.std(data2)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data1: mean=50.303 stdv=4.426\n",
            "data2: mean=51.764 stdv=4.660\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75bl_MUH09Z0"
      },
      "source": [
        "data1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0zpcPfthIbf"
      },
      "source": [
        "We have showed the [**independent two-sample t-test**](https://en.wikipedia.org/wiki/Student%27s_t-test#Independent_two-sample_t-test) to check that the mean of two independent samples are equal. We assumed that the variances were identical for both samples. The null hypothesis of the test is that the means of two populations are equal. A rejection of this hypothesis indicates that there is sufficient evidence that the means of the populations are different (and in turn that the distributions are not equal). We can use the [`ttest_ind()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html) Scipy function to implement this test:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klJvySzzhZsE",
        "outputId": "41f77531-0e81-4b1b-d32e-f7fa3584dba9"
      },
      "source": [
        "from scipy.stats import ttest_ind\n",
        "\n",
        "stat, p = ttest_ind(data1, data2)\n",
        "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
        "# interpret\n",
        "alpha = 0.05\n",
        "if p > alpha:\n",
        "\tprint('The means of the populations are equal (fail to reject H0)')\n",
        "else:\n",
        "\tprint('The means of the populations are different (reject H0)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Statistics=-2.262, p=0.025\n",
            "The means of the populations are different (reject H0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QMsC9OW-idr"
      },
      "source": [
        "If the variances are not identical for both samples we must use a slightly modified test statistic ([Welch's t-test](https://en.wikipedia.org/wiki/Welch%27s_t-test)). To apply this test we can use previous `ttest_ind()` function setting the parameter `equal_var=False`. \n",
        "\n",
        "The two samples can be dependent. For example, the data samples may represent two independent measures or evaluations of the same object. These data samples are repeated or dependent and are referred to as paired samples or repeated measures. Because the samples are not independent, we cannot use the previous test. Instead, we must use a modified version of the test that corrects for the fact that the data samples are dependent, called the paired Student’s t-test or [dependent t-test for paired samples](https://en.wikipedia.org/wiki/Student%27s_t-test#Dependent_t-test_for_paired_samples). This stet can be implemented in python using the [`ttest_rel()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html) Scipy function.\n",
        "\n",
        "\n",
        "If the variances for both samples are known (and both populations are normal, or if the sample sizes are large enough so that the central limit theorem applies) then the hypothesis may be tested using a direct application of the normal distribution (the **two-sample Z-test**). \n",
        "\n",
        "Some experiments involve comparing only one population mean $\\mu$ to a specified value, say, $\\mu_0$ . The null hypothesis is $H_0: \\mu=\\mu_0$. If the population is normal with known variance, or if the population is nonnormal but the sample size is large enough so that the central limit theorem applies, then the hypothesis may be tested using a direct application of the normal distribution (the one-sample [Z-test](https://en.wikipedia.org/wiki/Z-test)). If the variance of the population is unknown, we must make the additional assumption that the population is normally distributed, although moderate departures from normality will not seriously affect the results. To test $H_0: \\mu=\\mu_0$ in the variance unknown case, the sample variance $S^2$ is used to estimate $\\sigma^2$ and we used the [one-sample t-test](https://en.wikipedia.org/wiki/Student%27s_t-test#One-sample_t-test). We can use the [`ttest_1samp()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_1samp.html) Scipy function.\n",
        "\n",
        "Tables 2.4 and 2.5 summarize the t-test and z-test procedures discussed above for sample means. Critical regions are shown for both two-sided and one-sided alternative hypotheses.\n",
        "\n",
        "![alt text](https://i.ibb.co/HXwd8dN/means1.png)\n",
        "\n",
        "![alt text](https://i.ibb.co/yfkL2zY/means2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkQuhsIAA2rv"
      },
      "source": [
        "### 2.1.1 Analysis of variance (ANOVA) <a name=\"2.1.1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UAkhrjUA2rv"
      },
      "source": [
        "There are sometimes situations where we may have multiple independent data samples. We can perform the Student’s t-test pairwise on each combination of the data samples to get an idea of which samples have different means. This can be onerous if we are only interested in whether all samples have the same distribution or not.\n",
        "\n",
        "The [Analysis of variance](https://en.wikipedia.org/wiki/Analysis_of_variance) (ANOVA) is a collection of statistical models and their associated estimation procedures (such as the \"variation\" among and between groups) used to analyze the differences among group means in a sample.\n",
        "\n",
        "ANOVA provides a statistical test of whether two or more population means are equal, and therefore generalizes the t-test beyond two means. It cannot quantify which samples differ or by how much. \n",
        "\n",
        "The test requires that the data samples are a Gaussian distribution, that the samples are independent, and that all data samples have the same standard deviation.\n",
        "\n",
        "The ANOVA one-factor test can be performed in Python using the [`f_oneway()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.f_oneway.html) SciPy function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh3u0bRvA2rv"
      },
      "source": [
        "#### 2.1.1.1 Post-hoc analysis <a name=\"2.1.1.1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sogBpI36A2rw"
      },
      "source": [
        "When the ANOVA indicates that row or column means differ, it is usually of interest to make comparisons between the individual row or column means to discover the specific differences.\n",
        "\n",
        "The [Scheffé’s Method](https://en.wikipedia.org/wiki/Scheff%C3%A9%27s_method) is used to compare any possible contrast between treatment means.\n",
        "\n",
        "**Comparing pairs of treatment means**: In many practical situations, we will wish to compare only pairs of means. Frequently, we can determine which means differ by testing the differences between all pairs of treatment means.\n",
        "\n",
        "- The [Tukey’s Test](https://en.wikipedia.org/wiki/Tukey%27s_range_test) is a single-step multiple comparison procedure and statistical test that can be used to find means that are significantly different from each other.\n",
        "\n",
        "\n",
        "- The **Fisher Least Significant Difference (LSD)** method is also used to compare all pairs of means."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBPVsbMd9_Lb"
      },
      "source": [
        "## 2.2 Inferences about the variances <a name=\"2.2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSpFI0E3BBV-"
      },
      "source": [
        "### 2.2.1 Inferences About the Variances of Normal Distributions <a name=\"2.2.1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhkkmDyTBIjX"
      },
      "source": [
        "In many experiments, we are interested in possible differences in the mean response for two samples. However, in some experiments, it is the comparison of variability in the data that is important.\n",
        "\n",
        "Suppose we wish to test the hypothesis that the variance of a normal population equals a constant, for example, $\\sigma_0^2$. Formally, we wish to test\n",
        "$$H_o:\\sigma^2=\\sigma_0^2$$\n",
        "$$H_o:\\sigma^2\\neq \\sigma_0^2$$\n",
        "\n",
        "We can use the [Chi-squared test for variance in a normal population](https://en.wikipedia.org/wiki/Chi-squared_test#Chi-squared_test_for_variance_in_a_normal_population). The test statistic is \n",
        "$$\\chi_o^2=\\frac{SS}{\\sigma_0^2}=\\frac{(n-1)S^2}{\\sigma_0^2}$$\n",
        "\n",
        "where $SS=\\sum{}{}\\sum{}{}(y_{i}-\\overline{y})^2$ is  the  corrected  sum  of  squares  of  the  sample  observations.  The appropriate reference distribution for $\\chi_o^2$ is the chi-square distribution with $n-1$ degrees of freedom. The null hypothesis is rejected if $\\chi_o^2>\\chi_{\\frac{\\alpha}{2},n-1}^2$.\n",
        "\n",
        "Now consider testing the equality of the variances of two normal populations. If independent random samples of size $n1$ and $n2$ are taken from populations 1 and 2, respectively,the test statistic for\n",
        "$$H_o:\\sigma_1^2=\\sigma_2^2$$\n",
        "$$H_o:\\sigma_1^2\\neq \\sigma_2^2$$\n",
        "is the ratio of the sample variances $F_0=\\frac{S_1^2}{S_2^2}$\n",
        "\n",
        "The appropriate reference distribution for $F_0$ is the $F$ distribution with $n_1-1$ numerator degrees of freedom and $n_2-1$ denominator degrees of freedom. The null hypothesis would be  rejected  if  \n",
        "$$F_0>F_{\\frac{\\alpha}{2},n_1-1,n_2-1}$$ \n",
        "\n",
        "**Note**: This test is called [F-test of equality of variances](https://en.wikipedia.org/wiki/F-test_of_equality_of_variances). The [F-test](https://en.wikipedia.org/wiki/F-test) is used in a variety of tests including regression analysis, the Chow test, and the Scheffe Test (a post-hoc ANOVA test). \n",
        "\n",
        "\n",
        "![alt text](https://i.ibb.co/Z1jKJpZ/gggggg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L56xKqPuEKoo"
      },
      "source": [
        "### 2.2.2 Statistical tests for equality of variance <a name=\"2.2.2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZ4wO9R_EOR5"
      },
      "source": [
        "Equal variances across populations is called homoscedasticity or homogeneity of variances. Some statistical tests, for example the analysis of variance, assume that variances are equal across groups or samples. The following tests can be used to verify that assumption.\n",
        "\n",
        "[Bartlett’s test](https://en.wikipedia.org/wiki/Bartlett%27s_test) is used to test if $k$ samples are from populations with equal variances. The null hypothesis is that all input samples are from populations with equal variances.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rp6Zu_1iB_6w",
        "outputId": "f556a059-9631-4fd6-c31c-6c26d0f91b60"
      },
      "source": [
        "# Create three samples\n",
        "a = [8.88, 9.12, 9.04, 8.98, 9.00, 9.08, 9.01, 8.85, 9.06, 8.99]\n",
        "b = [8.88, 8.95, 9.29, 9.44, 9.15, 9.58, 8.36, 9.18, 8.67, 9.05]\n",
        "c = [8.95, 9.12, 8.95, 8.85, 9.03, 8.84, 9.07, 8.98, 8.86, 8.98]\n",
        "print([np.var(x, ddof=1) for x in [a, b, c]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.007054444444444413, 0.13073888888888888, 0.008890000000000002]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cbjtu4OMCC99"
      },
      "source": [
        "Given that the sample variance of `b` is much larger than that of `a` and `c`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6gE7bPDA8w3",
        "outputId": "f673072c-91dd-45d4-bb05-59f09fe9b9c5"
      },
      "source": [
        "from scipy.stats import bartlett\n",
        "\n",
        "stat, p = bartlett(a, b, c)\n",
        "print('stat=%.3f, p=%.3f' % (stat, p))\n",
        "if p > 0.05:\n",
        "\tprint('All input samples are from populations with equal variances (fail to reject H0)')\n",
        "else:\n",
        "\tprint('Not all input samples are from populations with equal variances (reject H0)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "stat=22.789, p=0.000\n",
            "Not all input samples are from populations with equal variances (reject H0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6k4Bt6xJBDMA"
      },
      "source": [
        "[Levene's test](https://en.wikipedia.org/wiki/Levene%27s_test) is an inferential statistic used to assess the equality of variances for a variable calculated for two or more groups."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21WuuWFACMcN",
        "outputId": "dffe06ff-d5cd-4587-88b5-6ea9001422df"
      },
      "source": [
        "from scipy.stats import levene\n",
        "\n",
        "stat, p = levene(a, b, c)\n",
        "print('stat=%.3f, p=%.3f' % (stat, p))\n",
        "if p > 0.05:\n",
        "\tprint('All input samples are from populations with equal variances (fail to reject H0)')\n",
        "else:\n",
        "\tprint('Not all input samples are from populations with equal variances (reject H0)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "stat=7.585, p=0.002\n",
            "Not all input samples are from populations with equal variances (reject H0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9Ls85i0-buk"
      },
      "source": [
        "## 2.3 Normality Tests <a name=\"2.3\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE1zLLYV-buk"
      },
      "source": [
        "In some statistical models like analysis of the variance and regression models, we need to check the normality assumption of the residuals. We can check this assumption by plotting a *histogram* of the residuals. If the assumption on the errors is satisfied, this plot should look like a sample from a normal distribution centered at zero. Unfortunately, with small samples, considerable fluctuation in the shape of a histogram often occurs, so the appearance of a moderate departure from normality does not necessarily imply a serious violation of the assumptions. Gross deviations from normality are potentially serious and require further analysis. Another extremely useful procedure is to construct a *normal probability plot* of the residuals.\n",
        "\n",
        "There are many statistical tests that we can use to quantify whether a sample of data looks as though it was drawn from a Gaussian distribution. Each test makes different assumptions and considers different aspects of the data. We will look at 3 commonly used tests in this section that we can apply to our own data samples.\n",
        "\n",
        "The [Shapiro–Wilk test](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test) evaluates a data sample and quantifies how likely it is that the data was drawn from a Gaussian distribution. It tests the null hypothesis that a sample came from a normally distributed population. In practice, the Shapiro-Wilk test is believed to be a reliable test of normality, although there is some suggestion that the test may be suitable for smaller samples of data, e.g. thousands of observations or fewer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22spCShtDFir",
        "outputId": "95069b42-66a1-4f56-99a7-fce2ed22c027"
      },
      "source": [
        "from scipy.stats import shapiro\n",
        "\n",
        "# seed the random number generator\n",
        "np.random.seed(1)\n",
        "\n",
        "# generate univariate observations\n",
        "data = 5 * np.random.randn(100) + 50\n",
        "\n",
        "stat, p = shapiro(data)\n",
        "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
        "\n",
        "alpha = 0.05\n",
        "if p > alpha:\n",
        "\tprint('Sample looks Gaussian (fail to reject H0)')\n",
        "else:\n",
        "\tprint('Sample does not look Gaussian (reject H0)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Statistics=0.992, p=0.822\n",
            "Sample looks Gaussian (fail to reject H0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-Ifd7FXCnyJ"
      },
      "source": [
        "The [Anderson-Darling test](https://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test) is a statistical test of whether a given sample of data is drawn from a given probability distribution. When applied to testing whether a normal distribution adequately describes a set of data, it is one of the most powerful statistical tools for detecting most departures from normality. The test is a modified version of a more sophisticated nonparametric goodness-of-fit statistical test called the [Kolmogorov-Smirnov test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1B22SW36CpJg",
        "outputId": "ddbdcac0-bde0-45e9-8e0a-519665bc4f9a"
      },
      "source": [
        "from scipy.stats import anderson\n",
        "\n",
        "result = anderson(data)\n",
        "print('Statistic: %.3f' % result.statistic)\n",
        "\n",
        "for i in range(len(result.critical_values)):\n",
        "\tsl, cv = result.significance_level[i], result.critical_values[i]\n",
        "\tif result.statistic < result.critical_values[i]:\n",
        "\t\tprint('%.3f: %.3f, data looks normal (fail to reject H0)' % (sl, cv))\n",
        "\telse:\n",
        "\t\tprint('%.3f: %.3f, data does not look normal (reject H0)' % (sl, cv))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Statistic: 0.220\n",
            "15.000: 0.555, data looks normal (fail to reject H0)\n",
            "10.000: 0.632, data looks normal (fail to reject H0)\n",
            "5.000: 0.759, data looks normal (fail to reject H0)\n",
            "2.500: 0.885, data looks normal (fail to reject H0)\n",
            "1.000: 1.053, data looks normal (fail to reject H0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTJX8J63EksJ"
      },
      "source": [
        "We interpret the results by failing to reject the null hypothesis that the data is normal if the calculated test statistic is less than the critical value at a chosen significance level."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqWPD-rhCpSK"
      },
      "source": [
        "The [D’Agostino’s K^2 test](https://en.wikipedia.org/wiki/D%27Agostino%27s_K-squared_test) calculates summary statistics from the data (kurtosis and skewness) to determine if the data distribution departs from the normal distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGY0v4TaDZpK",
        "outputId": "08307fdb-3ad0-437c-91a4-14b8f04f4342"
      },
      "source": [
        "from scipy.stats import normaltest\n",
        "\n",
        "stat, p = normaltest(data)\n",
        "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
        "\n",
        "alpha = 0.05\n",
        "if p > alpha:\n",
        "\tprint('Sample looks Gaussian (fail to reject H0)')\n",
        "else:\n",
        "\tprint('Sample does not look Gaussian (reject H0)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Statistics=0.102, p=0.950\n",
            "Sample looks Gaussian (fail to reject H0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtTP6lRzWrSZ"
      },
      "source": [
        "## 2.4 Nonparametric tests <a name=\"2.4\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvwQy2JKWrSa"
      },
      "source": [
        "[Nonparametric statistics](https://en.wikipedia.org/wiki/Nonparametric_statistics) are those methods that do not assume a specific distribution to the data. A common question about two or more datasets is whether they are different. Specifically, whether the difference between their central tendency (e.g. mean or median) is statistically significant.\n",
        "\n",
        "This question can be answered for data samples that do not have a Gaussian distribution by using nonparametric statistical significance tests. The null hypothesis of these tests is often the assumption that both samples were drawn from a population with the same distribution, and therefore the same population parameters, such as mean or median.\n",
        "\n",
        "If after calculating the significance test on two or more samples the null hypothesis is rejected, it indicates that there is evidence to suggest that samples were drawn from different populations, and in turn, the difference between sample estimates of population parameters, such as means or medians may be significant.\n",
        "\n",
        "The [Mann–Whitney U test](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) is a nonparametric statistical significance test for determining whether two independent samples were drawn from a population with the same distribution. The null hypothesis is that there is no difference between the distributions of the data samples. Rejection of this hypothesis suggests that there is likely some difference between the samples. More specifically, the test determines whether it is equally likely that any randomly selected observation from one sample will be greater or less than a sample in the other distribution. If violated, it suggests differing distributions. For the test to be effective, it requires at least 20 observations in each data sample.\n",
        "\n",
        "We will use the `randn()` NumPy function to generate a sample of 100 Gaussian random numbers in each sample with a mean of 0 and a standard deviation of 1. Observations in the first sample are scaled to have a mean of 50 and a standard deviation of 5. Observations in the second sample are scaled to have a mean of 51 and a standard deviation of 5.\n",
        "\n",
        "We expect the statistical tests to discover that the samples were drawn from differing distributions, although the small sample size of 100 observations per sample will add some noise to this decision.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMA7DzCzLRoB",
        "outputId": "244e3d60-0bb3-498f-a50d-fae073b9e186"
      },
      "source": [
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "# seed the random number generator\n",
        "np.random.seed(1)\n",
        "\n",
        "# generate two independent samples\n",
        "data1 = 5 * np.random.randn(100) + 50\n",
        "data2 = 5 * np.random.randn(100) + 51\n",
        "\n",
        "stat, p = mannwhitneyu(data1, data2)\n",
        "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
        "alpha = 0.05\n",
        "if p > alpha:\n",
        "\tprint('Same distribution (fail to reject H0)')\n",
        "else:\n",
        "\tprint('Different distribution (reject H0)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Statistics=4025.000, p=0.009\n",
            "Different distribution (reject H0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g39Y9FzQLMj0"
      },
      "source": [
        "In some cases, the data samples may be paired (the samples are related or matched in some way or represent two measurements of the same technique). Each sample is independent, therefore the Mann-Whitney U test cannot be used.\n",
        "The [Wilcoxon signed-rank test](https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test) is a nonparametric statistical procedure for comparing two samples that are paired, or related. The null hypothesis is that the two samples have the same distribution. For the test to be effective, it requires at least 20 observations in each data sample. This test is a [paired difference test](https://en.wikipedia.org/wiki/Paired_difference_test).\n",
        "\n",
        "\n",
        "The example below demonstrates the calculation of the Wilcoxon signed-rank test on the test problem. The two samples are technically not paired, but we can pretend they are for the sake of demonstrating the calculation of this significance test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRrcVysFL1vh",
        "outputId": "c1161862-8633-49a6-e55d-1020f6f3afbf"
      },
      "source": [
        "from scipy.stats import wilcoxon\n",
        "\n",
        "stat, p = wilcoxon(data1, data2)\n",
        "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
        "\n",
        "alpha = 0.05\n",
        "if p > alpha:\n",
        "\tprint('Same distribution (fail to reject H0)')\n",
        "else:\n",
        "\tprint('Different distribution (reject H0)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Statistics=1886.000, p=0.028\n",
            "Different distribution (reject H0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGEoKBLILOQK"
      },
      "source": [
        "When working with significance tests, such as Mann-Whitney U and the Wilcoxon signed-rank tests, comparisons between data samples must be performed pair-wise. This can be inefficient if we have many data samples and we are only interested in whether two or more samples have a different distribution. The [Kruskal–Wallis Test](https://en.wikipedia.org/wiki/Kruskal%E2%80%93Wallis_one-way_analysis_of_variance) is a nonparametric version of the one-way analysis of variance test (ANOVA). \n",
        "The null hypothesis is that all data samples were drawn from the same distribution. Specifically, that the population medians of all groups are equal. A rejection of the null hypothesis indicates that there is enough evidence to suggest that one or more samples dominate another sample, but the test does not indicate which samples or by how much. Each data sample must be independent, have 5 or more observations, and the data samples can differ in size.\n",
        "\n",
        "We can update the test problem to have 3 data samples, instead of 2, two of which have the same sample mean. Given that one sample differs, we would expect the test to discover the difference and reject the null hypothesis.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeQ7U3CyL5TH",
        "outputId": "413d3705-bdff-4aa5-a66f-c5c2a134f41a"
      },
      "source": [
        "from scipy.stats import kruskal\n",
        "\n",
        "np.random.seed(1)\n",
        "# generate three independent samples\n",
        "data1 = 5 * np.random.randn(100) + 50\n",
        "data2 = 5 * np.random.randn(100) + 50\n",
        "data3 = 5 * np.random.randn(100) + 52\n",
        "\n",
        "stat, p = kruskal(data1, data2, data3)\n",
        "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
        "# interpret\n",
        "alpha = 0.05\n",
        "if p > alpha:\n",
        "\tprint('Same distributions (fail to reject H0)')\n",
        "else:\n",
        "\tprint('Different distributions (reject H0)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Statistics=6.051, p=0.049\n",
            "Different distributions (reject H0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1juHBlx2LItW"
      },
      "source": [
        "We may have more than two different samples and interest in whether all samples have the same distribution or not. If the samples are paired in some way, such as repeated measures, then the Kruskal-Wallis H test would not be appropriate. The [Friedman test](https://en.wikipedia.org/wiki/Friedman_test) is the nonparametric version of the repeated measures analysis of variance test. The null hypothesis is that the multiple paired samples have the same distribution. A rejection of the null hypothesis indicates that one or more of the paired samples has a different distribution. The test assumes two or more paired data samples with 10 or more samples per group.\n",
        "\n",
        "Although the samples are not paired, we expect the test to discover that not all of the samples have the same distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U43nSUO4MRAD",
        "outputId": "a69690ea-725f-46d6-8b53-0822112f320f"
      },
      "source": [
        "from scipy.stats import friedmanchisquare\n",
        "\n",
        "stat, p = friedmanchisquare(data1, data2, data3)\n",
        "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
        "# interpret\n",
        "alpha = 0.05\n",
        "if p > alpha:\n",
        "\tprint('Same distributions (fail to reject H0)')\n",
        "else:\n",
        "\tprint('Different distributions (reject H0)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Statistics=9.360, p=0.009\n",
            "Different distributions (reject H0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sw6FaSecFT8a"
      },
      "source": [
        "The [Kolmogorov-Smirnov](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test) test is a non-parametric hypothesis test that can be used to check whether two samples come from the same continuous distribution. It measures a distance between the empirical distribution functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ocl8aYXFUj1",
        "outputId": "75958527-b6fc-4cf7-86f3-643f0a52a122"
      },
      "source": [
        "from scipy.stats import ks_2samp \n",
        "\n",
        "# seed the random number generator\n",
        "np.random.seed(1)\n",
        "\n",
        "# generate two independent samples\n",
        "data1 = 5 * np.random.randn(100) + 50\n",
        "data2 = 5 * np.random.randn(100) + 51\n",
        "\n",
        "stat, p = ks_2samp(data1, data2)\n",
        "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
        "\n",
        "alpha = 0.05\n",
        "if p > alpha:\n",
        "\tprint('Same distributions (fail to reject H0)')\n",
        "else:\n",
        "\tprint('Different distributions (reject H0)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Statistics=0.220, p=0.016\n",
            "Different distributions (reject H0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2K1QLCjN_M0"
      },
      "source": [
        "## 2.5 Correlation Tests <a name=\"2.5\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9tp2x2jdlkf"
      },
      "source": [
        "It can be useful in data analysis and modeling to better understand the relationships between variables. The statistical relationship between two variables is referred to as their correlation. A correlation could be positive, meaning both variables move in the same direction, or negative, meaning that when one variable’s value increases, the other variables’ values decrease. Correlation can also be neutral or zero, meaning that the variables are unrelated.\n",
        "\n",
        "The performance of some algorithms can deteriorate if two or more variables are tightly related, called [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity). An example is a linear regression, where one of the offending correlated variables should be removed in order to improve the skill of the model.\n",
        "\n",
        "We may also be interested in the correlation between input variables with the output variable in order to provide insight into which variables may or may not be relevant as input for developing a model.\n",
        "\n",
        "This section lists statistical tests that we can use to check if two samples are related. \n",
        "\n",
        "Before we look at correlation methods, let’s define a dataset we can use to test the methods.\n",
        "\n",
        "We will generate 1,000 samples of two variables with a strong positive correlation. The first variable will be random numbers drawn from a Gaussian distribution with a mean of 100 and a standard deviation of 20. The second variable will be values from the first variable with Gaussian noise added with a mean of 50 and a standard deviation of 10. We will use the `randn()` function to generate random Gaussian values with a mean of 0 and a standard deviation of 1, then multiply the results by our own standard deviation and add the mean to shift the values into the preferred range.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "Xd41O-HlRAc_",
        "outputId": "055ef5da-5e37-4559-a652-720c303bad2c"
      },
      "source": [
        "from matplotlib import pyplot\n",
        "# seed random number generator\n",
        "np.random.seed(1)\n",
        "# prepare data\n",
        "data1 = 20 * np.random.randn(1000) + 100\n",
        "data2 = data1 + (10 * np.random.randn(1000) + 50)\n",
        "# summarize\n",
        "print('data1: mean=%.3f stdv=%.3f' % (np.mean(data1), np.std(data1)))\n",
        "print('data2: mean=%.3f stdv=%.3f' % (np.mean(data2), np.std(data2)))\n",
        "# plot\n",
        "pyplot.scatter(data1, data2)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data1: mean=100.776 stdv=19.620\n",
            "data2: mean=151.050 stdv=22.358\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3L2tSOe1RoCm"
      },
      "source": [
        "The covariance and covariance matrix are used widely within statistics and multivariate analysis to characterize the relationships between two or more variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUO7kHNURdpC",
        "outputId": "c1a64b0d-348b-4536-c032-7fd1a5df0e6b"
      },
      "source": [
        "# calculate covariance matrix\n",
        "covariance = np.cov(data1, data2)\n",
        "print(covariance)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[385.33297729 389.7545618 ]\n",
            " [389.7545618  500.38006058]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpKwxyjYR4po"
      },
      "source": [
        "The covariance between the two variables is 389.75. We can see that it is positive, suggesting the variables change in the same direction as we expect. A problem with covariance as a statistical tool alone is that it is challenging to interpret. This leads us to the Pearson’s correlation coefficient next."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lxq4iCrgQ1HC"
      },
      "source": [
        "The [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) can be used to summarize the strength of the linear relationship between two data samples. It is computed as the covariance of the two variables divided by the product of the standard deviation of each data sample. It is the normalization of the covariance between the two variables to give an interpretable score. The result of the calculation, the correlation coefficient can be interpreted to understand the relationship. The coefficient returns a value between -1 and 1 that represents the limits of correlation from a full negative correlation to a full positive correlation. A value of 0 means no correlation. The value must be interpreted, where often a value below -0.5 or above 0.5 indicates a notable correlation, and a value below those values suggests a less notable correlation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hroNToeKSCXj",
        "outputId": "f630aad9-8909-457c-d964-ac8778da1217"
      },
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "# calculate Pearson's correlation\n",
        "corr, _ = pearsonr(data1, data2)\n",
        "print('Pearsons correlation: %.3f' % corr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pearsons correlation: 0.888\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZC7e3t4SMUC"
      },
      "source": [
        "We can see that the two variables are positively correlated and that the correlation is `0.88`.\n",
        "\n",
        "We can compute a matrix of the relationships between each pair of variables in the dataset. The result is a symmetric matrix called a correlation matrix with a value of 1.0 along the diagonal as each column always perfectly correlates with itself. The `corr()` method of Pandas DataFrames computes pairwise correlation of columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxmebM50Qlgk"
      },
      "source": [
        "Two variables may be related by a nonlinear relationship, such that the relationship is stronger or weaker across the distribution of the variables. In this case, the [Spearman's rank correlation coefficient](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient) can be used to summarize the strength between the two data samples. This test of relationship can also be used if there is a linear relationship between the variables but will have slightly less power. As with the Pearson correlation coefficient, the scores are between -1 and 1 for perfectly negatively correlated variables and perfectly positively correlated respectively. Instead of calculating the coefficient using covariance and standard deviations on the samples themselves, these statistics are calculated from the relative rank of values on each sample. A linear relationship between the variables is not assumed, although a monotonic relationship (an increasing or decreasing relationship) is assumed.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5keboczS4N-",
        "outputId": "8b03f83b-0807-41fa-fbc0-c9260cf5aff2"
      },
      "source": [
        "from scipy.stats import spearmanr\n",
        "\n",
        "# calculate spearman's correlation\n",
        "corr, _ = spearmanr(data1, data2)\n",
        "print('Spearmans correlation: %.3f' % corr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spearmans correlation: 0.872\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAx5W-sEQm9K"
      },
      "source": [
        "The [Kendall rank correlation coefficient](https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient) is often used as a test statistic in a statistical hypothesis test to establish whether two variables may be regarded as statistically dependent. The intuition for the test is that it calculates a normalized score for the number of matching or concordant rankings between the two samples. As a statistical hypothesis test, the null hypothesis is that there is no association between the two samples.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTpzsBBITd_S",
        "outputId": "0ea6df0c-c895-4768-c8ed-6b8b48f262e2"
      },
      "source": [
        "from scipy.stats import kendalltau\n",
        "\n",
        "coef, p = kendalltau(data1, data2)\n",
        "print('Kendall correlation coefficient: %.3f, p:%.3f' % (coef,p))\n",
        "alpha = 0.05\n",
        "if p > alpha:\n",
        "\tprint('Samples are uncorrelated (fail to reject H0)')\n",
        "else:\n",
        "\tprint('Samples are correlated (reject H0)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Kendall correlation coefficient: 0.688, p:0.000\n",
            "Samples are correlated (reject H0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjxnWIk7QoEF"
      },
      "source": [
        "The [Pearson's chi-squared test](https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test) is a statistical hypothesis test that assumes (the null hypothesis) that the observed frequencies for a categorical variable match the expected frequencies for the categorical variable. The test calculates a statistic that has a chi-squared distribution and can be interpreted to reject or fail to reject the assumption that the observed and expected frequencies are the same. The variables are considered independent if the observed and expected frequencies are similar, that the levels of the variables do not interact, are not dependent.\n",
        "\n",
        "The `chi2_contingency()` SciPy function takes an array as input representing the contingency table for the two categorical variables. It returns the calculated statistic and p-value for interpretation as well as the calculated degrees of freedom and table of expected frequencies. \n",
        "\n",
        "A contingency table is defined below that has a different number of observations for each population (row) but a similar proportion across each group (column). Given the similar proportions, we would expect the test to find that the groups are similar and that the variables are independent (fail to reject the null hypothesis, $H_0$). The calculated expected frequency table is printed and we can see that indeed the observed contingency table does appear to match via a check of the numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9k6kO9abUz14",
        "outputId": "9f0e0942-d483-4094-c83c-b1f8cc71dfea"
      },
      "source": [
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# contingency table\n",
        "contingency_table = [[10, 20, 30],\n",
        "\t\t             [6,  9,  17]]\n",
        "\n",
        "# Chi-square test of independence.\n",
        "c, p, dof, expected = chi2_contingency(contingency_table)\n",
        "print('Expected frequency table:\\n', expected)\n",
        "alpha=0.05\n",
        "if p <= alpha:\n",
        "    print('Dependent (reject H0)')\n",
        "else:\n",
        "    print('Independent (fail to reject H0)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Expected frequency table:\n",
            " [[10.43478261 18.91304348 30.65217391]\n",
            " [ 5.56521739 10.08695652 16.34782609]]\n",
            "Independent (fail to reject H0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPFdio5PZVMY"
      },
      "source": [
        "**Note**: We can compute the contingency table of two categorical variables in a dataset using the `crosstab()` Pandas function.\n",
        "\n",
        "**Note**: Once a machine learning model is trained and deployed in production, there are two approaches to monitor its performance degradation: ground truth evaluation or input drift detection. The logic of input drift detection is that if the data distribution (e.g., mean, standard deviation, correlations between features, etc.) diverges between the training and testing phases on one side and the development phase on the other, it is a strong signal that the model’s performance won’t be the same. This method requires, for each feature, applying a statistical test on data from the source distribution and the target distribution.\n",
        "\n",
        "- For continuous features, the nonparametric Kolmogorov-Smirnov test is mostly used.\n",
        "\n",
        "- For categorical features, the Pearson's chi-squared test is a practical choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFMFaF8wUYMm"
      },
      "source": [
        "# References <a name=\"3\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJCPKzOYGC9K"
      },
      "source": [
        "- Nonparametric Statistics for Non-Statisticians: A Step-by-Step\n",
        "\n",
        "- Design and analysis of experiments\n",
        "\n",
        "- [Statistical hypothesis tests in python](https://machinelearningmastery.com/statistical-hypothesis-tests-in-python-cheat-sheet/)"
      ]
    }
  ]
}