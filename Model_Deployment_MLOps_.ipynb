{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model Deployment MLOps .ipynb",
      "provenance": [],
      "collapsed_sections": [
        "oNqFbXrEop9S",
        "r23bZ79Td1vs",
        "pPPc76Fu8TgP",
        "JK9obT498_LE",
        "Ze9vy7sUnirR",
        "szeTmDq0CWjv",
        "P3Ri-aObEE-t",
        "hHEBQoZwF7N0",
        "oj85u3hC9l91",
        "9xz4N0Gl9o96",
        "4AHMZy6_GfyQ",
        "i0nGDJDIHKTA",
        "n8-1rw4XXmRW",
        "6nIDiqh8YFcR",
        "bZatEMbioDVC",
        "SFSHbwyzYRO6",
        "QMx2T7J7YjCI",
        "rcjjxLyUPzSU",
        "JTsrFEPkY81E",
        "bXGtmZ14ZELN",
        "JVmSRv15ZVOu",
        "o-1-aZyl9AO-"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM6jrU19Vk9qsEuLFojndbn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorviro/Machine-Learning-Python/blob/master/Model_Deployment_MLOps_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czjzVuKdk4Jw"
      },
      "source": [
        "# ğŸš€ ML model deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNqFbXrEop9S"
      },
      "source": [
        "# Table of contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOunBijSo16s"
      },
      "source": [
        "1. [â„¹ Introduction](#1)\n",
        "2. [â¬…ï¸ Before production](#2)\n",
        "    1. [âš™ Runtyme environments](#2.1)\n",
        "    2. [â›”ï¸ Data access](#2.2)\n",
        "    3. [ğŸ§ Model risk evaluation](#2.3)\n",
        "    4. [âœ” QA and Auditability for ML](#2.4)\n",
        "    5. [ğŸ”’ ML security](#2.5)\n",
        "        1. [ğŸ˜ˆ Adversarial attacks](#2.5.1)\n",
        "        2. [ğŸ”¥ Other vulnerabilities](#2.5.2)\n",
        "    6. [â¬‡ Model risk mitigation](#2.6)\n",
        "3. [ğŸš€ Deploying to production](#3)\n",
        "    1. [ğŸ‘·â€â™‚ï¸ CI/CD](#3.1)\n",
        "        1. [âœ… Testing](#3.1.1)\n",
        "    2. [â†”ï¸ ML pipeline](#3.2)\n",
        "    3. [ğŸš€ Model deployment](#3.3)\n",
        "        1. [ğŸššğŸ Batch vs online inference ](#3.3.1)\n",
        "        2. [ğŸ‘‰ Model deployment strategies](#3.3.2)\n",
        "        3. [ğŸ”§ Maintenance in production](#3.3.3)\n",
        "    4. [ğŸ“¦ Containerization](#3.4)\n",
        "    5. [â¬† Scaling deployments](#3.5)\n",
        "4. [ğŸ“• References](#4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r23bZ79Td1vs"
      },
      "source": [
        "# â„¹ Introduction <a name=\"1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJ_0gm2q8TWB"
      },
      "source": [
        "Productionalizing and ğŸš€ **deploying models** and ML pipelines is a ğŸ”‘ key component of MLOps that presents **different** technical **challenges than developing the model**. It's the **domain of the software/ML engineer and the DevOps** team, and the organizational challenges in managing the information exchange between the data scientists and these teams must not be underestimated. As described in the notebook [Introduction to MLOps](https://nbviewer.jupyter.org/github/victorviro/Machine-Learning-Python/blob/master/Introduction_to_MLOps.ipynb), without effective ğŸ”— collaboration between the teams, âŒ›ï¸ delays or ğŸ‘ failures to deploy are inevitable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPPc76Fu8TgP"
      },
      "source": [
        "# â¬…ï¸ Before production <a name=\"2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EunOON9w8Xd-"
      },
      "source": [
        "> That something works in the ğŸ§ª lab does not mean it will work well in the real ğŸŒ world. \n",
        "\n",
        "The **production environment is typically different from the development environment**, and the **commercial risks** associated with models **in production are â¬† greater**. The complexities of the â© transition to production need to be understood and tested, and the potential risks adequately mitigated.\n",
        "\n",
        "This section explores some important **considerations to prepare for production** in robust MLOps systems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JK9obT498_LE"
      },
      "source": [
        "## âš™ Runtime environments <a name=\"2.1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j13QSTEE9AG_"
      },
      "source": [
        "The 1ï¸âƒ£ first step in ğŸš€ **deploying a model** to production is making sure **itâ€™s technically possible**. **Ideal** MLOps systems favor âœˆ **rapid, ğŸ¤– automated deployment** over labor-intensive processes. **Production environments take a wide variety of forms**: custom-built services, data science platforms, dedicated services like TensorFlow Serving, low-level infrastructure like Kubernetes clusters, etc.\n",
        "\n",
        "Ideally, models running in the development environment would be âœ… validated and â¡ï¸ sent as is to production; thus â¬‡ minimizing the amount of adaptation work and improving the chances that the model in production will behave as it did in development. Unfortunately, this **ideal scenario is not always possible**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY36aCKV9rBt"
      },
      "source": [
        "**Adaptation from development to production environment**\n",
        "\n",
        "**If** the development and production platforms are ğŸ”„ interoperable, and **the model can run without any modification in production**, then the **steps required to push the model into production** are reduced to **a few ğŸ–±ï¸ clicks or commands**, and all efforts can be focused on âœ… validation.\n",
        "\n",
        "However, there are **cases** (the reality in many organizations) **where the model needs to be â†©ï¸ reimplemented** from scratch, possibly by another team, and possibly in another programming language (that model wonâ€™t probably reach production for months). In these cases, there can be **model transformations or interactions with the environment to make the model compatible with production**. In all cases, it is crucial to **perform validation in an environment that mimics production as closely as possible**.\n",
        "\n",
        "- The **format required** to send to production should be ğŸ¤” considered â¬… early, as it **may impact** on the model itself and the work required to productionalize it. For example, when a model is developed using Python and production is a Java environment that expects PMML or ONNX as input, â†ªï¸ conversion is required.\n",
        "\n",
        "- Performance is important when the production model must **run on a low-power ğŸ“± device** (Edge AI). With ğŸ§  deep neural networks, for example, trained models can be extremely large. One solution is to use ğŸ—œ compression techniques like quantization, pruning, or distillation. These methods are still recent but already used in NLP."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## â›”ï¸ Data access <a name=\"2.2\"></a>"
      ],
      "metadata": {
        "id": "Ze9vy7sUnirR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "um7oYR04BryS"
      },
      "source": [
        "Another aspect to address before validation and launch to ğŸš€ production is â›”ï¸ data access. For example, a model predicting ğŸ  apartment ğŸ’° prices may use the average market price in a zip code area; however, the ğŸ‘¨ user requesting the scoring will probably not provide this average and would provide simply the zip code, meaning **a lookup is necessary** to fetch the value of the average.\n",
        "\n",
        "**In some cases**, data can be frozen and bundled with the model. But when this is not possible (e.g., if the dataset is too large or the enrichment data needs to be ğŸ“… up to date), the **production environment should access a database** and thus have the appropriate **network ğŸ–‡ connectivity, libraries, or ğŸš™ drivers required to communicate with the data storage** installed, and authentication ğŸ” credentials stored in some form of production ğŸ”§ configuration. Managing this setup and configuration can be complex in practice since it requires appropriate âš’ï¸ tooling and collaboration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szeTmDq0CWjv"
      },
      "source": [
        "## ğŸ§ Model risk evaluation <a name=\"2.3\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIfUb7lUCsSb"
      },
      "source": [
        "**Implementation of models can have ğŸ› bugs**, as can the environment they are executing in.  It is possible to â¬… **anticipate the risks of models in production** and thus design and â˜‘ï¸ validate them to â¬‡ **minimize these risks**.\n",
        "\n",
        "Before putting a model in ğŸš€ production teams should **ask the next â“ questions**:\n",
        "\n",
        "- What if the model acts in the ğŸ˜Ÿ worst imaginable way?\n",
        "- What if a user manages to extract the training data or the internal logic of the model?\n",
        "- What are the financial, business, legal, safety, and reputational âš ï¸ risks?\n",
        "\n",
        "For âš¡ï¸ **high-risk applications**, the whole team must be ğŸ§ aware of these risks so that they can design the âœ” validation process appropriately and apply the complexity appropriate for the magnitude of the risks.\n",
        "\n",
        "ML model risk originates essentially from:\n",
        "\n",
        "- ğŸ› Bugs in designing, training, or evaluating the model\n",
        "- Bugs in the runtime framework, in the model post-processing/conversion, or hidden incompatibilities between the model and its runtime\n",
        "- â¬‡ Low quality of training data\n",
        "- High difference between production data and training data\n",
        "- Adversarial ğŸ˜ˆ attacks\n",
        "- Reputational risk due to bias, unethical use of ML, etc.\n",
        "\n",
        "The risk and its ğŸš magnitude can be â¬† amplified by:\n",
        "\n",
        "- Broad use of the model\n",
        "- A ğŸ rapidly changing environment\n",
        "- Complex â›“ interactions between models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3Ri-aObEE-t"
      },
      "source": [
        "## âœ” QA and auditability for ML <a name=\"2.4\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmDvKE8EFxUY"
      },
      "source": [
        "Software engineering has developed ğŸ›  tools and methodologies for quality assurance (QA), but the equivalent for ML is still in its ğŸ‘¶ infancy. The ğŸ¯ **purpose of QA for ML** is to **ensure compliance with âš™ processes as well as ML and computational performance requirements**, with a level of detail appropriate to the ğŸš level of âš ï¸ risk.\n",
        "\n",
        "The organizationâ€™s structure needs to give people in charge of validation the ğŸ‘® authority to appropriately ğŸ“ report ğŸ› issues, contribute to continuous improvement, and âœ‹ block passage to production if the level of risk justifies it. Performing QA before sending to production is not only about technical validation, it is also the occasion to create âœï¸ documentation and validate the model against organizational ğŸ“‹ guidelines. The origin of all input datasets, pre-trained models, or other assets should be known, as they could be subject to ğŸ“œ regulations or copyrights.\n",
        "\n",
        "**Auditability** is related to reproducibility (which we discussed in the notebook [Model development in MLOps](https://nbviewer.jupyter.org/github/victorviro/Machine-Learning-Python/blob/master/Model_development_in_MLOps.ipynb)), but it adds some requirements. For a model to be auditable, it must be possible to **access the full history (lineage) of the ML pipeline from a central and reliable storage** and to easily fetch metadata on all model versions including:\n",
        "\n",
        "- The full ğŸ““ documentation\n",
        "- An artifact that allows running the model with its exact initial environment\n",
        "- âœ” Test results, including model explanations and fairness reports\n",
        "- Detailed model ğŸ”Š logs and ğŸ–¥ monitoring metadata\n",
        "\n",
        "Auditability can be required in â¬† highly regulated applications, but it has benefits for all organizations because it **facilitates model ğŸ debugging, continuous improvement, and keeping âœï¸ track of actions and responsibilities** (which is an essential part of governance for responsible AI). A full QA ğŸ§° toolchain for ML should provide a clear view of model performance about requirements while also facilitating auditability.\n",
        "\n",
        "Auditability must allow for an intuitive human ğŸ¤” understanding of all the parts of the system and their version histories. Depending on the âš¡ criticality of the application, a wider audience may need to understand the details of the model. As a result, ğŸ”‹ full auditability comes at a cost that should be âš–ï¸ balanced with the criticality of the model itself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHEBQoZwF7N0"
      },
      "source": [
        "## ğŸ”’ ML security <a name=\"2.5\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwIrjD6gF9qa"
      },
      "source": [
        "As a piece of software, **a ğŸš€ deployed model can present security ğŸ› issues**. ML introduces a new range of potential threats where an **ğŸ˜ˆ attacker provides malicious data designed to cause the model to make a ğŸ‘ mistake**.\n",
        "\n",
        "There are numerous cases of potential ğŸ’¥ attacks. For **example, spam filters** based on scoring words that were in a dictionary. One way for spam creators to avoid detection was to avoid âœï¸ writing these words while still making their message easily understandable by a ğŸ‘¨ human reader (e.g., using exotic Unicode characters, introducing typos or ğŸ–¼ images)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ˜ˆ Adversarial attacks <a name=\"2.5.1\"></a>"
      ],
      "metadata": {
        "id": "oj85u3hC9l91"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfS8b9utGDSp"
      },
      "source": [
        "\n",
        "\n",
        "A more modern example of an ML model security issue is an **adversarial attack for ğŸ§  deep neural networks** in which an ğŸ§‘ image **slight modification** that can seem minor or even impossible for a human ğŸ‘ eye to notice **can cause the model to drastically change its prediction**. Since deep learning inference is essentially matrix multiplication, carefully chosen small perturbations to coefficients can cause a large change in the output numbers.\n",
        "\n",
        "<center><img src='https://i.ibb.co/QbYkDF5/adversarial-attack.png'></center>\n",
        "\n",
        "For complex models like deep learning, the ğŸ˜ˆ attacker will probably need to perform many queries and either use ğŸ’ª brute force to test as many combinations as possible or use a model to search for problematic examples. The **difficulty of countermeasures is â¬† increasing with the complexity of models** and their availability. Simple models such as logistic regressions are essentially immune, while an open-source pre-trained deep neural network will always be vulnerable, even with advanced attack detectors.\n",
        "\n",
        "Adversarial attacks donâ€™t necessarily happen at inference time. If an attacker can get **access to the training data**, even partially, then they get ğŸ•¹ control over the system. This kind of attack is traditionally known as a â˜ ï¸ **poisoning attack** in computer security. One famous **example** is the **Twitter ğŸ¤– chatbot** released by Microsoft in 2016. Just a few â³ hours after launch, the bot **started to generate ğŸ–• offensive tweets**. This was caused by the bot adapting to its input; when realizing that some users submitted a large amount of offensive content, the bot started to replicate. In theory, a poisoning attack can occur as a result of an intrusion or even, in a more sophisticated way, through pre-trained models. But in practice, one should mostly care about data collected from easily manipulated data sources. Tweets sent to a specific account are a particularly clear example."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ”¥ Other vulnerabilities <a name=\"2.5.2\"></a>"
      ],
      "metadata": {
        "id": "9xz4N0Gl9o96"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6SLS-ppGThM"
      },
      "source": [
        "Since the **ML models** can be considered a ğŸ“„ summary of the data they have been trained on, they **can leak** more or less ğŸ‘Œ precise **information on the training data**. Imagine, for example, that a model predicts how much someone is paid using the nearest neighbor algorithm. If one knows the zip code, age, and profession of a certain person registered on the service, itâ€™s pretty easy to obtain that personâ€™s exact ğŸ’² income. There is a wide range of attacks that can extract information from models in this way.\n",
        "\n",
        "In addition to technical hardening and audit, **governance** is critical in security. **Responsibilities** must be ğŸ“ assigned clearly and in an appropriate âš–ï¸ balance between security and capacity of execution. It is also important to put in place ğŸ”„ **feedback âš™ mechanisms**, and employees and users should have an easy **channel to ğŸ”‰ communicate breaches** (including programs that ğŸ reward reporting vulnerabilities). It is also possible, and necessary, to build safety nets around the system to mitigate the risks.\n",
        "\n",
        "One of the main ideas is that computer system **security is not an â• additional independent feature of the system**; that is, generally we cannot secure a system that is not designed to be secure, and the organization processes must take into account the nature of the threat from the beginning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AHMZy6_GfyQ"
      },
      "source": [
        "## â¬‡ Model risk mitigation <a name=\"2.6\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVcPAW7ZGhbC"
      },
      "source": [
        "To ğŸ•¹ control the ğŸš€ deployment of new versions, progressive or **canary deployments** allow ğŸ†• **new versions of models being served to a small proportion of the customer base 1ï¸âƒ£ first and slowly ğŸ“ˆ increasing that proportion**, while ğŸ“Š monitoring behavior and getting human ğŸ”„ feedback if appropriate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEwzrTxoGms5"
      },
      "source": [
        "**Changing environments**\n",
        "\n",
        "ğŸ Rapidly **changing environments â¬† multiply risk**. Even with an efficient monitoring system and a âš™ procedure to ğŸ” retrain models, the âŒ› time necessary to remediate may be a critical threat, especially if simply retraining the model on new data is not enough and a new model must be ğŸ§‘â€ğŸ’» developed. During this time, the production systems misbehaving can cause large ğŸ’° losses for the organization.\n",
        "\n",
        "To control this risk, monitoring should be reactive enough (â° alerting on distributions computed every week might not be enough), and the procedure should consider the period necessary for remediation. For example, in addition to retraining or â†©ï¸ rollout strategies, the procedure may define ğŸ”¢ **thresholds that would trigger a degraded mode for the system**. A degraded mode may simply consist of a âš ï¸ warning message displayed for end-users but could be as drastic as shutting down the system to avoid harm until a stable solution can be deployed.\n",
        "\n",
        "In many cases, ğŸ” retraining the model on more data will â¬† improve the model, and this problem will disappear, but this can take â³ time. Before this convergence, a solution might be to use a simpler model that may have a â¬‡ lower performance but be more consistent in a frequently changing environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SW5D4E4_GvX0"
      },
      "source": [
        "ğŸ–‡ **Interactions between models**\n",
        "\n",
        "Complex interactions between models are a challenging source of risk. â• **Adding models will often add complexity** to an organization, but the complexity **does not necessarily grow linearly in proportion to the number of models**. The absence of interactions between models makes the complexity grow closer to linearly (in practice, it is rarely the case, as there can always be interactions in the real ğŸŒ world even if models are not connected).\n",
        "\n",
        "The total complexity is determined by how the ğŸ”— interactions with models are designed. Using **models in â›“ chains (where a model uses inputs from another model) can create additional complexity** as well as unexpected results, **whereas using models independently**, which are each as short and explainable as possible, **is** a much **more â™»ï¸ sustainable** way to design the large-scale deployment of ML. On other hand, [ensembled models](https://nbviewer.org/github/victorviro/ML_algorithms_python/blob/master/Ensemble_learning.ipynb) can avoid errors, that is, if a decision is based on several independent models with methods as different as possible, it can be more robust."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EzL3mSnHBKh"
      },
      "source": [
        "**Model misbehavior**\n",
        "\n",
        "Many ğŸ“ measures can be implemented to avoid model misbehavior. \n",
        "\n",
        "ğŸ•¹ Controlling **feature-value intervals** is a **useful** and simple technique. For example, if the value of a **feature at inference time is out of bounds, the system can trigger appropriate measures** (e.g., dispatching a âš ï¸ warning message). But **it might be insufficient**. For example, when training an algorithm to evaluate ğŸš™ carğŸ’² prices, the data may have provided examples of recent light cars and old heavy cars, but no recent heavy cars. The performance of a model for these is unpredictable. When the number of features is large, this issue becomes unavoidable due to the [curse of dimensionality](https://nbviewer.org/github/victorviro/ML_algorithms_python/blob/master/Dimensionality_reduction_algorithms.ipynb#The-Curse-of-Dimensionality).\n",
        "\n",
        "In these situations, more sophisticated methods can be used, including **anomaly detection** to identify records where the model is used outside of its application domain. After scoring, the outputs of the model can be ğŸ§ examined before âœ… confirming the inference. In the case of ğŸ· classification, many algorithms provide certainty scores in addition to their prediction, and a ğŸ”¢ threshold can be fixed to accept an inference output.\n",
        "\n",
        "**Conformal prediction** is a set of âš’ï¸ techniques that helps âš–ï¸ calibrate these predictions to obtain an accurate estimation of the probability of correctness. For regression, the value can be âœ… checked against a predetermined interval. For example, if the model predicts a car costs between 50\\$ and 500,000\\$, we may not want to âŒ commit any business on this prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0nGDJDIHKTA"
      },
      "source": [
        "# ğŸš€ Deploying to production <a name=\"3\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL7I16B09AJx"
      },
      "source": [
        "ğŸš€ **Deploying to production** is a ğŸ— key component of MLOps, and, having the right âš™ processes and ğŸ§° tools can ensure that it happens âœˆ **quickly**. The good ğŸ“° news is that many of the elements of success in software engineering, particularly **CI/CD best practices**, **can be applied to ML**.\n",
        "\n",
        "This section dives into the concepts and considerations when ğŸš€ deploying ML pipelines and models to production."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8-1rw4XXmRW"
      },
      "source": [
        "## ğŸ‘·â€â™‚ï¸ CI/CD <a name=\"3.1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0I0XuVPXs7B"
      },
      "source": [
        "Let's understand some DevOps concepts:\n",
        "\n",
        "- ***Integration***: The process of **merging a contribution to a central repository** (typically merging a Git ğŸŒ¿ branch to the main branch) **and performing âœ… tests**.\n",
        "\n",
        "- ***Delivery***: The process of **building a fully ğŸ“¦ packaged and validated version of the software ready to be ğŸš€ deployed** to production.\n",
        "\n",
        "- ***Deployment***: The process of **updating and ğŸ”› running a new version of the software on a ğŸ¯ target infrastructure**. \n",
        "\n",
        "- ***Release***: **A release is a version of the software**. **Releasing** is the act of **making the release version ğŸ‘€ visible to our customers**. ğŸš€ Deployment doesn't necessarily expose customers to the ğŸ†• new version of the software, although in many cases, it is implicit. But, there are ways to hide the new version from customers even while it's deployed (like in blue/green deployment or shadow testing). The goal of releasing is ensuring a feature meets customer needs and is ğŸ”™ turned off when defective.\n",
        "\n",
        "[***CI/CD***](https://en.wikipedia.org/wiki/CI/CD) is an acronym for **continuous integration and continuous delivery**, a philosophy of agile software development, and a set of ğŸ“œ practices and ğŸ›  tools to **release applications more often and âœˆ faster**, while also better ğŸ•¹ controlling quality and risk. They are a critical part of MLOps strategy.\n",
        "\n",
        "- The goal of **continuous integration** is **merge, frequently, the work from several contributors into a shared repository** (using a version control system). Each integration can then be âœ… verified by **automated tests** (automated build). While automated testing is not strictly part of CI, it is typically implied. By integrating regularly, ğŸ› errors can be detected quickly and located more easily. It allows checking that the application is not broken whenever new commits are integrated into the main branch.\n",
        "\n",
        "- **Continuous delivery** is an extension of CI since it ğŸ¤– automatically deploys the new version of the software to a testing and/or production-like environment after the build stage to ensure the software will ğŸ‘ work in production. So, the **software can be reliably released to production at any time**. Beyond making sure our application passes automated tests, it has to have all the configuration necessary to push it into production. \n",
        "\n",
        "- Continuous delivery is sometimes confused with continuous deployment. **Continuous deployment** goes one step further than continuous delivery. It means that **every change goes through the pipeline and automatically gets put into production**. There's no âœ‹ human intervention, and only a â failed test will prevent a new change to be ğŸš€ deployed to production. Continuous delivery just means that we can do frequent deployments but we may choose not to do it. Fully ğŸ¤– automated deployment is not always desirable and is a business decision as much as a technical decision. A further explanation about the differences between continuous delivery and continuous deployment is available in this [video](https://youtu.be/LNLKZ4Rvk8w).\n",
        "\n",
        "<center><img src='https://i.ibb.co/hcrdszd/cd-cd.png'></center>\n",
        "\n",
        "The most **widespread ğŸ›  tool for CI/CD** is [***Jenkins***](https://www.jenkins.io/), an open-source system that allows for the building of CI/CD pipelines regardless of the programming language, testing framework, etc. Jenkins can be used in data science to orchestrate CI/CD pipelines, although there are many other options (like [Github Actions](https://github.com/features/actions)).\n",
        "\n",
        "An â¬† **incremental approach to building a CI/CD pipeline** is natural since a starting project has no infrastructure ğŸ“‹ requirements of a tech giant, and it can be hard to know â¬…ï¸ upfront which challenges deployments will present. The best ğŸ›£ï¸ path is starting from a simple (but fully functional) CI/CD workflow and introducing more sophisticated steps along the way as quality or scaling challenges appear."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nIDiqh8YFcR"
      },
      "source": [
        "### âœ… Testing <a name=\"3.1.1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRVZYR1bObWl"
      },
      "source": [
        "âœ… Testing is an important strategy for improving reliability, â¬‡ reducing technical debt and lowering long-term ğŸ”§ maintenance costs. However, as suggested in the following figure (from [The ML Test Score](https://research.google/pubs/pub46555/)), **ML system testing is more complex than testing traditional software systems**, since **ML system behavior** is not specified directly in code but **is learned from data**.\n",
        "\n",
        "<center><img src='https://i.ibb.co/2WF825x/ML-testing-monitoring.png'></center>\n",
        "\n",
        "Therefore, while traditional software can rely on **unit tests and integration tests of the code**, for ML we need to â• **add tests for the data, and the model**.\n",
        "\n",
        "- **Data validation** is required â¬… before model training to decide whether we should retrain the model or stop the execution of the pipeline. This decision can be automatically made based on the **data schema or data values skews**.\n",
        "\n",
        "- **Model validation** occurs â¡ after we successfully train the model given the new data, and â¬… before it's promoted to ğŸš€ production. In the notebook [Model development in MLOps](https://nbviewer.jupyter.org/github/victorviro/Machine-Learning-Python/blob/master/Model_development_in_MLOps.ipynb), we discussed how to evaluate a model properly. Some additional steps for this **offline model validation** are:\n",
        "\n",
        " - **Comparing** the evaluation ğŸ“ metric values produced by our ğŸ†• newly trained model **to the production model** (or previous versions) or to a performance ğŸ”¢ threshold (does the new model produce better performance than the current model?).\n",
        "\n",
        " - Check the **consistent performance of the model on** various segments of the data (**subpopulations**).\n",
        "\n",
        " - Making sure that we test our model for deployment, including infrastructure compatibility and consistency with the prediction service API.\n",
        "\n",
        "Good tests should make it as **easy** as possible **to diagnose the source ğŸ› issue when they â fail**. For that purpose, **naming appropriately the tests is important**.\n",
        "\n",
        "**Online validation**: In addition to offline validation, some âœ… checks should be performed and automatically analyzed when the model is already ğŸš€ deployed to production (canary deployment or A/B testing).\n",
        "\n",
        "- An essential part of model validation is testing on recent production data. One or several datasets should be used, extracted from several time windows, and named appropriately. \n",
        "-  Online validation also should monitor computational performance (CPU, memory, disk, latency, etc).\n",
        "\n",
        "**ğŸ¤– Automating these tests** as much as possible **is essential** and a ğŸ”‘ key component of efficient MLOps. A lack of automation or ğŸš„ speed wastes âŒ› time, but also it ğŸ˜ discourages the development team from testing and deploying often, which can ğŸ• delay the discovery of ğŸ› bugs or design choices that make it impossible to deploy to production."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZatEMbioDVC"
      },
      "source": [
        "## â†”ï¸ ML pipeline <a name=\"3.2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL5MxGFzoE0d"
      },
      "source": [
        "**In many businesses** that are ğŸŒ± beginning to apply ML to their use cases, their **process for building and deploying ML models is entirely ğŸ¤š manual**. This process can separate data scientists who create the model and engineers who serve the model as a prediction service, and it presents a **disconnection between ML and operations**. In practice, models often break when they are deployed in the real ğŸŒ world (they ğŸ‘ fail to adapt to changes in the dynamics of the environment or changes in the data that describes the environment).\n",
        "\n",
        "To address the challenges of this manual process, MLOps practices for CI/CD are helpful. By ğŸš€ **deploying an ML training pipeline**, we can set up a CI/CD system to âœˆ rapidly âœ… test, ğŸ— build, and ğŸš€ deploy new implementations of the ML pipeline. ğŸ¤– Automated data validation and model validation steps must be added to the ML pipeline.\n",
        "\n",
        "<center><img src='https://i.ibb.co/bKLDmN8/automatic-ml-pipeline.png'></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFSHbwyzYRO6"
      },
      "source": [
        "## ğŸš€ Model deployment <a name=\"3.3\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8mV7FL4DJ69"
      },
      "source": [
        "The process of **taking a trained ML model and making its predictions available** to ğŸ§‘ users or other systems is known as model ğŸš€ deployment in ML. To decide **how to deploy a model**, we need to ğŸ¤” understand **how end-users should interact with the modelâ€™s predictions**. There are **multiple â“ factors to consider** when determining how to deploy an ML model. These factors include:\n",
        "\n",
        "- How ğŸ•› frequently predictions should be generated?\n",
        "- Predictions should be generated for a 1ï¸âƒ£ single instance at a time or a batch of instances?\n",
        "- The number of applications that will access the model.\n",
        "- The â³ latency requirements of these applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMx2T7J7YjCI"
      },
      "source": [
        "### ğŸššğŸ Batch inference vs online inference <a name=\"3.3.1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwzStxr_YlHZ"
      },
      "source": [
        "In addition to different deployment strategies, there are two ways to approach model ğŸš€ deployment:\n",
        "\n",
        "- ğŸšš **Batch or offline inference** generates predictions on a **batch of observations**. The batch jobs are typically generated on some **recurring schedule** (e.g. ğŸ•‘ **hourly, ğŸ“… daily**). **Latency is often not a concern**. These predictions are then stored in a database and can be made available to ğŸ§‘â€ğŸ’» developers or end-users. Batch inference may use big data ğŸ§° tools such as Spark to generate predictions on large batches. Some advantages are:\n",
        "\n",
        " - We can âœ… check and verify all of our predictions â¬… before they are used.\n",
        "\n",
        " - Once the predictions have been âœï¸ written to some look-up table, they can be served with minimal â³ latency (no feature computation or model inference needs to be done at request time).\n",
        "\n",
        " <center><img src='https://i.ibb.co/m4RbBs7/batch-inference.png'></center>\n",
        "\n",
        "- ğŸ **Real-time or online inference** generates predictions in âœˆ real-time upon request. **Typically**, these predictions are generated **on a 1ï¸âƒ£ single observation** of data at runtime. For example, when an ad is displayed on a website and a user session is scored by models to decide what to display.\n",
        "\n",
        "  <center><img src='https://i.ibb.co/c60bKn5/online-inference.png'></center>\n",
        "\n",
        "\n",
        "In both cases, multiple instances of the model can be ğŸš€ deployed to â¬† increase throughput and lower â³ latency.\n",
        "\n",
        "Typically, **online inference faces more challenges** than batch inference. Online inference tends to be more complex because of the added ğŸ§° tooling and systems required to meet latency requirements. A system that needs to respond with a prediction within 100ms is much harder to implement than a system with a service-level agreement of 24 hours. In those 100ms, the system needs to â¬…ï¸ retrieve any necessary data to generate predictions, perform âš™ inference, âœ… validate the model output, and then (typically) â†©ï¸ return the results over a network.\n",
        "\n",
        "One of the first â“ questions weâ€™ll need to answer when deciding how to deploy our ML models is whether to use **batch inference or online inference**. This ğŸ—³ choice is mainly driven by product **factors**: \n",
        "- Who is using the inferences, and how âœˆ soon do they need them? \n",
        "- If the predictions do not need to be served immediately, we may opt for the simplicity of ğŸšš batch inference. \n",
        "- If predictions need to be served on an individual basis and within the time of a single web request, ğŸ online inference is the way to go."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcjjxLyUPzSU"
      },
      "source": [
        "### ğŸ‘‰ Model deployment strategies <a name=\"3.3.2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FTwIZnTYtcJ"
      },
      "source": [
        "When â¡ï¸ sending a new model version to production, the first consideration is often to **avoid downtime**, in particular for ğŸ online inference. \n",
        "\n",
        "- [***Blue-green deployment***](https://en.wikipedia.org/wiki/Blue-green_deployment): Rather than shutting down the system, upgrading it, and then putting it back online, the **new system** can be **deployed and tested in another environment (green)**, that is as identical as possible to the stable production environment (blue). Note that it's ğŸš€ deployed but it's not released, since it does not start responding to production requests. **Once the software is working in the green environment, the live ğŸš¦ traffic is ğŸ”› switched** so that all incoming requests go to the green environment. If no ğŸ› issues are found for a period of time, the ğŸ”µ blue environment can be âš°ï¸ removed.\n",
        "\n",
        " <center><img src='https://i.ibb.co/J7thY91/blue-green-deployment.png'></center>\n",
        "\n",
        "- ***Canary deployment***. The stable version of the model is kept in production, but a **small percentage of the workload is â¡ redirected to the new model, and results are ğŸ–¥ monitored**. Other ğŸ§‘ users continue to use the previous version until **weâ€™re ğŸ‘ satisfied with the new release. Then, we can gradually roll the new release out to all users** (potentially in several workload percentage â¬† increments). This way, a ğŸ› malfunction would likely impact only a small portion of the workload. Computational performance and statistical âœ” tests can be performed to decide whether to fully ğŸ”› switch to the new model.\n",
        "\n",
        " <center><img src='https://i.ibb.co/6N6326V/canary-release1.png'></center>\n",
        " <center><img src='https://i.ibb.co/KX0NT0j/canary-release2.png'></center>\n",
        "\n",
        " Note that **requests handled by the canary model should be carefully ğŸ‘‰ picked**. For example, if the canary model is serving to a region, the model may, for ML or infrastructure reasons, not perform as expected in other regions. A more robust approach is to pick the portion of ğŸ§‘ users served by the new model at random, but then it is often desirable for user experience to implement an affinity âš™ mechanism so that the same user always uses the same version of the model.\n",
        "\n",
        " Canary testing can be used to carry out A/B âœ… testing, which is a process to compare two versions of an application in terms of a business performance ğŸ“ metric. A/B testing and shadow testing will be discussed in the following notebook.\n",
        "\n",
        " Overall, canary releases are a powerful âš’ï¸ tool, but they require advanced ğŸ§°ï¸ tooling to manage the ğŸš€ deployment, gather the ğŸ“ metrics, specify and run computations on them, display the results, and dispatch and process â° alerts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTsrFEPkY81E"
      },
      "source": [
        "### ğŸ”§ Maintenance in production <a name=\"3.3.3\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do3uFlAbY_xu"
      },
      "source": [
        "Once a model is released, it must be ğŸ”§ maintained. At a high level, there are three maintenance ğŸ“ measures:\n",
        "\n",
        "- ğŸ’» **Resource monitoring**: Just as for any application running on a server, collecting **IT metrics** such as **CPU, memory, disk, or network usage** can be useful to detect and troubleshoot ğŸ› issues.\n",
        "\n",
        "- âœ… **Health check**: To check if the model is indeed online and to analyze its â³ **latency**, it is common to implement a health check âš™ mechanism that simply queries the model at a fixed interval (on the order of one minute) and âœï¸ logs the results.\n",
        "\n",
        "- ğŸ“ **ML metrics monitoring**: This is about analyzing the **accuracy** of the model and **comparing it to another version or detecting when it is going stale**. Since it may require heavy computation, this is typically lower frequency, but as always, will depend on the application; it is typically done once a week.\n",
        "\n",
        "\n",
        "Finally, **when a ğŸ› malfunction is detected, a â†©ï¸ rollback to a previous version may be necessary**. It is critical to have the rollback procedure ready and as ğŸ¤– automated as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXGtmZ14ZELN"
      },
      "source": [
        "## ğŸ“¦ Containerization <a name=\"3.4\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfS5ZDinZG2s"
      },
      "source": [
        "Managing the versions of a model is more than **saving its code into a version control system**. It's necessary to provide a ğŸ“„ **description of the environment** (**libraries and their versions**, **ğŸ–‡ dependencies**, etc). But storing this information is not enough. Deploying to production should ğŸ¤– automatically and reliably ğŸ” **rebuild this environment on the target ğŸ’» machine**. The target machine may run multiple models simultaneously, and two models may have incompatible dependency versions. Several models running on the same machine could compete for resources, and one misbehaving model could hurt the performance of multiple cohosted models.\n",
        "\n",
        "ğŸ“¦ **Containerization** technology is used to **tackle these challenges**. These tools bundle an application together with its ğŸ”© configuration files, libraries, and ğŸ”— dependencies that are required for it to run across different operating environments. Unlike virtual machines, containers do not duplicate the operating system; and are therefore far more ğŸ‘ efficient.\n",
        "\n",
        "The most known containerization technology is the open-source platform ğŸ‹ [***Docker***](https://www.docker.com/). It allows an application to be ğŸ“¦ packaged, â¡ sent to a server, and run with all its dependencies in isolation from other applications.\n",
        "\n",
        "Building the basis of a model-serving environment that can accommodate many models, each of which may run multiple copies, may imply some issues:\n",
        "\n",
        "- Which Docker host(s) should receive the container?\n",
        "- When a model is ğŸš€ deployed in several copies, how can the workload be âš–ï¸ balanced?\n",
        "- What happens if the model becomes unresponsive, for example, if the machine hosting it âŒ fails? How can that be ğŸ§ detected and a container reprovisioned?\n",
        "- How can a model running on multiple machines be upgraded, with assurances that old and new versions are switched on and off, and that the load balancer is updated with a correct sequence?\n",
        "\n",
        "â˜¸ï¸ [***Kubernetes***](https://kubernetes.io/es/) is an open-source platform for **container orchestration**, which greatly simplifies these issues and many others. It provides a powerful declarative API to run applications in a group of Docker hosts, called a **Kubernetes cluster**. The word declarative means that rather than trying to express in code the steps to set up, ğŸ–¥ monitor, upgrade, âœ‹ stop, and connect the container (which can be complex and error-prone), users specify in a ğŸ“„ **configuration file** the desired state, and Kubernetes makes it happen and then maintains it.\n",
        "\n",
        "For example, users need only specify to Kubernetes \"make sure four instances of this container run at all times\", and Kubernetes will allocate the hosts, start the containers, monitor them, and start a new instance if one of them ğŸ‘ fails. \n",
        "\n",
        "Docker with Kubernetes can provide a powerful ğŸ— infrastructure to host applications, including ML models. Leveraging these products greatly simplifies the implementation of the ğŸš€ deployment strategies, like blue-green deployments or canary releases, although they are not aware of the nature of the deployed applications and thus canâ€™t natively manage the ML performance ğŸ“Š analysis. Another advantage is the ability to easily â¬† scale the modelâ€™s deployment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVmSRv15ZVOu"
      },
      "source": [
        "## â¬† Scaling deployments <a name=\"3.5\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UGmBODvZYi8"
      },
      "source": [
        "As ML adoption â¬† grows, organizations face two types of challenges:\n",
        "\n",
        "- The ability to use a model in production with high-scale data\n",
        "\n",
        "- The ability to train larger and larger numbers of models\n",
        "\n",
        "- Handling more data **for online inference** is easier with frameworks such as Kubernetes. Models can be replicated in the cluster in as many copies as necessary. The ğŸ¤–â¬† **auto-scaling** features in Kubernetes can provide new machines and load âš–ï¸ balancing. The major difficulty can then be to process a large amount of monitoring data.\n",
        "\n",
        "- **For batch scoring**, the situation can be more complex. When the volume of data becomes too large, there are essentially two types of strategies to â¡ï¸ distribute the computation:\n",
        "\n",
        " - Using a ğŸ§° framework that handles **distributed computation** natively like [**Spark**](https://spark.apache.org/). Spark is a distributed âš™ computation framework that **can split the data and the computation among its nodes**.\n",
        "\n",
        " - Another way to distribute batch processing is to âœ‚ï¸ **partition the data**. The general idea is that inference is typically a row-by-row operation (each row is scored one by one), and the data can be split in some way so that several machines can each read a subset of the data and score a subset of the rows.\n",
        "\n",
        "This is also challenging in terms of governance and processes. â¬† **Scaling the number of models** means that the **CI/CD pipeline** must be able to **handle many ğŸš€ deployments**. As the number of models â¬† grows, the need for ğŸ¤– automation and governance grows, as human verification cannot necessarily be systematic or consistent. In some applications, it is possible to rely on fully ğŸ¤– automated continuous deployment if the âš ï¸ risks are controlled by automated âœ… validation, canary releases, etc. There can be ğŸ— infrastructure challenges since training, building models, validating on test data, etc., all need to be performed on clusters rather than on a single ğŸ’» machine. \n",
        "\n",
        "Also, with a â¬† higher number of models, the CI/CD pipeline of each model can vary, and if nothing is done, each team could develop its own CI/CD pipeline for each model. This is suboptimal from efficiency and governance perspectives. While some models may need specific validation pipelines, most projects can probably use common patterns. In addition, ğŸ”§ maintenance is more complex as it may become impractical to implement a new validation step since the pipelines would not share a common structure and would then be impossible to update safely. ğŸ–‡ Sharing practices and standardized pipelines can help limit complexity.\n",
        "\n",
        "**Scalable and elastic systems**\n",
        "\n",
        "A computational system is said to be horizontally scalable if it is possible to â• add more servers to expand its processing ğŸ’ª power. For example, a Kubernetes cluster can be expanded to ğŸ’¯ hundreds of machines. However, if a system includes only one machine, it may be challenging to incrementally upgrade it significantly, and at some point, a migration to a bigger machine or a horizontally scalable system will be required.\n",
        "\n",
        "An elastic system allows, in addition to being scalable, easy â• addition and âš°ï¸ removal of resources to match the compute requirements. For example, a Kubernetes cluster in the cloud can have an **auto-scaling** capability that ğŸ¤– automatically **adds machines when the cluster usage metrics are â¬† high and removes them when they are â¬‡ low**. In principle, elastic systems can â™»ï¸ optimize the usage of resources; they **automatically adapt to an increase in usage without the need to permanently provision resources that are rarely required**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-1-aZyl9AO-"
      },
      "source": [
        "# ğŸ“• References <a name=\"4\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [MLOps: Continuous delivery and automation pipelines in machine learning](https://cloud.google.com/solutions/machine-learning/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)\n",
        "\n",
        "- [Book \"Introducing MLOps\"](https://www.oreilly.com/library/view/introducing-mlops/9781492083283/)\n",
        "\n",
        "\n",
        "- [AWS machine-learning-lens](https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/model-deployment-approaches.html)\n",
        "\n",
        "- [MLinproduction blog](https://mlinproduction.com/)\n",
        "\n",
        "- [Continuous Delivery for Machine Learning](https://martinfowler.com/articles/cd4ml.html)\n",
        "\n",
        "- [The ML Test Score](https://research.google/pubs/pub46555/)\n",
        "\n",
        "- [Awesome MLOps repo](https://github.com/visenger/awesome-mlops)\n",
        "\n"
      ],
      "metadata": {
        "id": "DL9XZ9Aukbqx"
      }
    }
  ]
}